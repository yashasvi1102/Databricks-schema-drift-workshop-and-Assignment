{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e4cb90-993a-4928-a798-b55a783bdf83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import pipelines as pl\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# update this to point to your local test files or mounted volume\n",
    "volume_path = \"/Volumes/workspace/damg7370/datastore/json file/customer_*.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4880df6c-c1f7-4a22-8299-3d8f93c75211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create bronze streaming table (rescue mode)\n",
    "pl.create_streaming_table(\"demo_cust_bronze_sd_rescue\")\n",
    "\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_bronze_sd_rescue\",\n",
    "  name = \"demo_cust_bronze_sd_rescue_ingest_flow\"\n",
    ")\n",
    "def demo_cust_bronze_sd_rescue_ingest_flow():\n",
    "  df = (\n",
    "      spark.readStream\n",
    "           .format(\"cloudFiles\")\n",
    "           .option(\"cloudFiles.format\", \"json\")\n",
    "           .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "           .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "           .load(f\"{volume_path}\")\n",
    "  )\n",
    "  return df.withColumn(\"ingestion_datetime\", current_timestamp())\\\n",
    "           .withColumn(\"source_filename\", col(\"_metadata.file_path\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d9707f7-b4ea-41df-8405-10fd6ebf0be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to handle DATATYPE changes by consuming _rescued_data JSON map and casting to target types\n",
    "def process__rescue_data_datatype_change(df, target_schema: StructType):\n",
    "    # Parse the _rescued_data JSON to a MAP (Key,Value) type and store in _rescued_data_modified column\n",
    "    df = df.withColumn(\"_rescued_data_modified\", from_json(col(\"_rescued_data\"), MapType(StringType(), StringType())))\n",
    "    \n",
    "    for field in target_schema.fields:\n",
    "        data_type = field.dataType\n",
    "        column_name = field.name\n",
    "\n",
    "        # key exists in rescued map?\n",
    "        key_condition = expr(f\"_rescued_data_modified IS NOT NULL AND map_contains_key(_rescued_data_modified, '{column_name}')\")\n",
    "        \n",
    "        # If rescued key exists, take it and cast to target type; otherwise use existing column (cast).\n",
    "        rescued_value = when(key_condition, col(\"_rescued_data_modified\").getItem(column_name).cast(data_type))\\\n",
    "                         .otherwise(col(column_name).cast(data_type))\n",
    "        \n",
    "        df = df.withColumn(column_name, rescued_value)\n",
    "        df = df.withColumn(column_name, col(column_name).cast(data_type))\n",
    "        \n",
    "    df = df.drop('_rescued_data_modified')\n",
    "\n",
    "    # set _rescued_data to null after processing (keeps expect_all_or_drop checks simple)\n",
    "    df = df.withColumn('_rescued_data', lit(None).cast(StringType()))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to extract new fields (present in _rescued_data) and add them as columns (string typed)\n",
    "def process__rescue_data_new_fields(df):\n",
    "    # parse JSON to map\n",
    "    df = df.withColumn(\n",
    "        \"_rescued_data_json_to_map\", \n",
    "        from_json(col(\"_rescued_data\"), MapType(StringType(), StringType()))\n",
    "    )\n",
    "\n",
    "    # get keys per row\n",
    "    df = df.withColumn(\"_rescued_data_map_keys\", map_keys(col(\"_rescued_data_json_to_map\")))\n",
    "\n",
    "    # get distinct keys across dataframe (non-streaming safe). if streaming, maintain a static list externally.\n",
    "    df_keys = df.select(explode(map_keys(col(\"_rescued_data_json_to_map\"))).alias(\"rescued_key\")).distinct()\n",
    "\n",
    "    new_keys = [row[\"rescued_key\"] for row in df_keys.collect()] if not df.isStreaming else []\n",
    "\n",
    "    for key in new_keys:\n",
    "        if key != \"_file_path\":\n",
    "            # add column using rescued map\n",
    "            df = df.withColumn(key, col(\"_rescued_data_json_to_map\").getItem(key).cast(StringType()))\n",
    "    # drop helper column\n",
    "    df = df.drop(\"_rescued_data_json_to_map\", \"_rescued_data_map_keys\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce9f22a5-8a14-44fb-aa2e-5e611dcb7115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: updated type we expect for signupDate (simulate datatype change)\n",
    "updated_datatypes = StructType([\n",
    "  StructField(\"signupDate\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Create silver streaming table (rescue version)\n",
    "pl.create_streaming_table(\n",
    "  name = \"demo_cust_silver_sd_rescue\",\n",
    "  expect_all_or_drop = {\"no_rescued_data\": \"_rescued_data IS NULL\",\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_silver_sd_rescue\",\n",
    "  name = \"demo_cust_silver_sd_rescue_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_sd_rescue_clean_flow():\n",
    "  df = spark.readStream.table(\"demo_cust_bronze_sd_rescue\")\n",
    "  # add any new fields that were rescued\n",
    "  df = process__rescue_data_new_fields(df)\n",
    "  # handle datatype change for columns defined in updated_datatypes\n",
    "  df = process__rescue_data_datatype_change(df, updated_datatypes)\n",
    "  # drop _rescued_data or keep it null (we set it null inside function)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8506ffe-b582-4171-abf6-51f37e1dffcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When using Auto Loader, schemaEvolutionMode = \"rescue\" is defensive: unexpected fields are stored in _rescued_data so the canonical schema stays stable and data engineers can choose when and how to promote new fields and handle datatype changes. schemaEvolutionMode = \"addNewColumns\" is aggressive: it mutates the schema by creating new top-level columns automatically, which is convenient when producers are stable but risky when producers are noisy or untrusted. In our experiment, rescue kept the bronze schema unchanged and placed new fields into _rescued_data for programmatic resolution; addNewColumns added the new fields directly to the table schema. Datatype changes were safer to handle with rescue since we could parse and cast values deterministically; with addNewColumns datatype changes caused inference conflicts or required additional error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8480003-6ce0-4daf-84dd-3da1df4b3213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create separate bronze table for 'addNewColumns' experiment\n",
    "pl.create_streaming_table(\"demo_cust_bronze_sd_addnew\")\n",
    "\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_bronze_sd_addnew\",\n",
    "  name = \"demo_cust_bronze_sd_addnew_ingest_flow\"\n",
    ")\n",
    "def demo_cust_bronze_sd_addnew_ingest_flow():\n",
    "  df = (\n",
    "      spark.readStream\n",
    "           .format(\"cloudFiles\")\n",
    "           .option(\"cloudFiles.format\", \"json\")\n",
    "           .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "           .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "           .load(f\"{volume_path}\")\n",
    "  )\n",
    "  return df.withColumn(\"ingestion_datetime\", current_timestamp()).withColumn(\"source_filename\", col(\"_metadata.file_path\"))\n",
    "\n",
    "# Silver for addNewColumns\n",
    "pl.create_streaming_table(\n",
    "  name = \"demo_cust_silver_sd_addnew\",\n",
    "  expect_all_or_drop = {\"valid_id\": \"CustomerID IS NOT NULL\"}\n",
    ")\n",
    "\n",
    "@pl.append_flow(\n",
    "  target = \"demo_cust_silver_sd_addnew\",\n",
    "  name = \"demo_cust_silver_sd_addnew_clean_flow\"\n",
    ")\n",
    "def demo_cust_silver_sd_addnew_clean_flow():\n",
    "  # Here we do NOT need to parse _rescued_data (it won't exist for new fields), but we still may need to handle datatype changes (which may fail)\n",
    "  df = spark.readStream.table(\"demo_cust_bronze_sd_addnew\")\n",
    "  # optionally: cast fields to expected types if you know schema\n",
    "  df = df.withColumn(\"signupDate\", to_date(col(\"signupDate\")))  # example\n",
    "  return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "schema_drift_part1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}