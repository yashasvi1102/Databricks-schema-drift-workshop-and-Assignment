{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b53524e-1ff0-4244-9fab-20ae1ef16bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0957edc1-0556-45f6-8b09-ebc9a19e3e55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "@dlt.table(\n",
    "    name=\"demo_cust_bronze_sd\",\n",
    "    comment=\"Bronze: raw JSON ingestion (cloudFiles) with schemaEvolutionMode=rescue\"\n",
    ")\n",
    "def bronze():\n",
    "    # Adjust the path below to your monitored folder if needed\n",
    "    input_path = \"/Volumes/workspace/damg7370/datastore/json file\"\n",
    "    return (\n",
    "        spark.readStream\n",
    "             .format(\"cloudFiles\")\n",
    "             .option(\"cloudFiles.format\", \"json\")\n",
    "             .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "             .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "             .load(input_path)\n",
    "             .withColumn(\"ingestion_datetime\", current_timestamp())\n",
    "             .withColumn(\"source_filename\", col(\"_metadata.file_path\"))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e68d1995-6cd9-4965-af21-a9a87ce8a4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# safe cast helper: treats empty string as null, does to_date conversion for DateType\n",
    "def safe_cast_col(col_expr, target_type):\n",
    "    if isinstance(target_type, DateType):\n",
    "        return when(trim(col_expr) == \"\", None).otherwise(to_date(trim(col_expr)))\n",
    "    else:\n",
    "        return when(trim(col_expr) == \"\", None).otherwise(col_expr.cast(target_type))\n",
    "\n",
    "# apply datatype changes based on provided StructType\n",
    "def process_rescued_datatype_changes(df, target_schema: StructType):\n",
    "    # parse _rescued_data to map if present\n",
    "    if \"_rescued_data\" in df.columns:\n",
    "        df = df.withColumn(\"_rescued_map\", from_json(col(\"_rescued_data\"), MapType(StringType(), StringType())))\n",
    "    else:\n",
    "        df = df.withColumn(\"_rescued_map\", lit(None).cast(MapType(StringType(), StringType())))\n",
    "\n",
    "    for field in target_schema.fields:\n",
    "        column_name = field.name\n",
    "        data_type = field.dataType\n",
    "\n",
    "        key_condition = (col(\"_rescued_map\").isNotNull()) & (map_keys(col(\"_rescued_map\")).isNotNull())  # conservative\n",
    "        # prefer rescued value if exists, otherwise existing column value\n",
    "        rescued_val = col(\"_rescued_map\").getItem(column_name)\n",
    "        merged_raw = when(rescued_val.isNotNull(), rescued_val).otherwise(col(column_name))\n",
    "        df = df.withColumn(column_name, safe_cast_col(merged_raw, data_type))\n",
    "\n",
    "    df = df.drop(\"_rescued_map\")\n",
    "    # clear _rescued_data after processing\n",
    "    if \"_rescued_data\" in df.columns:\n",
    "        df = df.withColumn(\"_rescued_data\", lit(None).cast(StringType()))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e64b4e7-64cf-4462-914b-6489262b9d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Define which fields we want to coerce to a specific type (example: signupDate -> Date)\n",
    "updated_datatypes = StructType([ StructField(\"signupDate\", DateType(), True) ])\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"demo_cust_silver_sd\",\n",
    "    comment=\"Silver: cleaned records with new fields promoted and signupDate cast to date\"\n",
    ")\n",
    "def silver():\n",
    "    bronze_df = dlt.read(\"demo_cust_bronze_sd\")\n",
    "\n",
    "    # If _rescued_data exists, extract keys in this DLT query definition (allowed)\n",
    "    discovered_keys = []\n",
    "    if \"_rescued_data\" in bronze_df.columns:\n",
    "        # Build a map column to extract keys (works within DLT dataset def)\n",
    "        rescued_map_col = from_json(col(\"_rescued_data\"), MapType(StringType(), StringType()))\n",
    "        keys_df = bronze_df.select(explode(map_keys(rescued_map_col)).alias(\"rescued_key\")) \\\n",
    "                           .distinct() \\\n",
    "                           .filter(col(\"rescued_key\").isNotNull())\n",
    "\n",
    "        # collect discovered keys (this is executed at pipeline runtime inside the dataset definition)\n",
    "        discovered_keys = [r[\"rescued_key\"] for r in keys_df.collect()]\n",
    "\n",
    "    # If no discovered keys, return bronze after datatype processing (still applied)\n",
    "    df = bronze_df\n",
    "\n",
    "    # Add discovered fields as columns (string typed by default)\n",
    "    if discovered_keys:\n",
    "        # parse rescued map once and then add new columns from it\n",
    "        df = df.withColumn(\"_rescued_map\", from_json(col(\"_rescued_data\"), MapType(StringType(), StringType())))\n",
    "        for k in discovered_keys:\n",
    "            if k is None or k == \"_file_path\":\n",
    "                continue\n",
    "            if k not in df.columns:\n",
    "                df = df.withColumn(k, col(\"_rescued_map\").getItem(k).cast(StringType()))\n",
    "        df = df.drop(\"_rescued_map\")\n",
    "\n",
    "    # Apply datatype changes (e.g., signupDate -> DateType)\n",
    "    df = process_rescued_datatype_changes(df, updated_datatypes)\n",
    "\n",
    "    # final cleanup: ensure _rescued_data is cleared\n",
    "    if \"_rescued_data\" in df.columns:\n",
    "        df = df.withColumn(\"_rescued_data\", lit(None).cast(StringType()))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfb2442b-7aac-4de1-97d5-b77d61442b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5 (optional)\n",
    "# Example DLT expectations: CustomerID should be present\n",
    "@dlt.expect_or_drop(\"valid_customerid\", \"CustomerID IS NOT NULL\")\n",
    "def silver_with_expectations():\n",
    "    return dlt.read(\"demo_cust_silver_sd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e54bdf-47c0-4d43-aff9-570165175827",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bfe8679-808e-428b-8798-877c9f40f152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44ff76d1-f4df-4be8-a1ca-a0328596a9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Part2codefixing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}